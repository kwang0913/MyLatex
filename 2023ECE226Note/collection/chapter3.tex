\chapter{Discrete Random Variables}

\section{Probability Mass Function (PMF)}
\begin{definition}
    The probability mass function (PMF) of a discrete random variable $X$ is
    \begin{equation*}
        P_X(x) = \P{X=x}.
    \end{equation*}
\end{definition}

\begin{theorem}
    [Properties of PMF]
    For random variable $X$, the PMF $P_X(x)$ has the following properties:
    \begin{enumerate}
        \item $P_X(x)\geq 0$.
        \item $\sum_{x\in S_X}P_X(x)=1$.
    \end{enumerate}
\end{theorem}

\section{Histogram}

\section{Families of Discrete Random Variables}
\begin{enumerate}
    \item Bernoulli($p$): Single experiment with success rate $p$ (\textbf{\ie, Flip a coin}), $x$ is the number of successes{
        \[ P_X(x) =
        \begin{cases}
            1-p & x=0, \\
            p   & x=1, \\
            0   & otherwise.
        \end{cases} \]
    }
    \item Binomial($n$, $p$): A sequence of $\mathbf{n}$ independent Bernoulli($p$) experiments, $x$ is the number of successes{
        \[P_X(\textcolor{red}{x}) = {
            \begin{cases}
                \binom{n}{\textcolor{red}{x}}p^x(1-p)^{n-x} & x=0,1,\ldots,n, \\
                0 & otherwise.
            \end{cases}
        }\]
        \textbf{Note:} Binomial($1, p$) $\iff$ Bernoulli($p$).

        \textbf{Note:} Binomial($n, p$) $\iff$ Independent trails.
    }
    \item Poisson($\alpha$): Binomial($n, p$) with $n\rightarrow\infty$, and $\alpha=np$, $x$ is the number of successes{
        \[ P_X(x) =
        \begin{cases}
            \dfrac{\alpha^x e^{-\alpha}}{x!}   & x=0,1,\ldots, \\
            0   & otherwise.
        \end{cases} \]
    }
    \item Geometric($p$): Get the $\bm{1}$\textbf{st} success at the $\bm{x}$\textbf{th} independent Bernoulli($p$) experiment {
        \[ P_X(x) =
        \begin{cases}
            p(1-p)^{x-1} & x=1,2,\ldots, \\
            0   & otherwise.
        \end{cases} \]
    }
    \item Pascal($k,p$): Get the $\bm{k}$\textbf{th} success at the $\bm{x}$\textbf{th} independent Bernoulli($p$) experiment {
        \[P_X(\textcolor{red}{x}) = {
            \begin{cases}
                \binom{\textcolor{red}{x}-1}{k-1}p^k(1-p)^{x-k} & x=k,k+1,k+2,\ldots, \\
                0 & otherwise.
            \end{cases}
        }\]
        \textbf{Note:} Pascal($k,p$) is also called Negative Binomial($k,p$).

        \textbf{Note:} Pascal($1, p$) $\iff$ Geometric($p$).

        \textbf{Note:} Pascal($k, p$) is a sequence of $\mathbf{k}$ independent Geometric($p$) experiments.
    }
    \item Discrete Uniform($k, l$): outcomes are uniformly distributed on range $(k, l)$ \textbf{\ie, Roll a Die}{
        \[ P_X(x) =
        \begin{cases}
            1/(l-k+1)   & x=k,k+1,k+2,\ldots,l, \\
            0   & otherwise.
        \end{cases} \]
    }
\end{enumerate}

\section{Cumulative Distribution Function (CDF)}
\begin{definition}
    The cumulative distribution function (CDF) of a discrete random variable $X$ is
    \begin{align*}
        F_X(x)&=P_X[X\leq x]=\sum_{k=0}^{x}P_X(k). \\
        F_X(b)-F_X(a)&=\sum_{k=0}^{b}P_X(k)-\sum_{k=0}^{a}P_X(k)=\sum_{k=a+1}^{b}P_X(k)=P_X(a<X\leq b).
    \end{align*}
\end{definition}

\begin{theorem}
    [The CDF of Geometric ($p$) is worth to remember]
    \begin{align*}
        F_X(x)
        &= P_X[X\leq x] \\
        &= 1-P_X[X > x] \\
        &= 1-\sum_{i=x+1}^{\infty}p(1-p)^{i-1} \\
        &= 1-(1-p)^x\sum_{i=1}^{\infty}p(1-p)^{i-1} \\
        &= 1-(1-p)^x.
    \end{align*}
\end{theorem}

\section{Expected Value}
\begin{definition}
    [Average]
    In ordinary language, an \textbf{Average} is a single number taken as representative of a list of numbers.
    \begin{enumerate}
        \item Mode: The outcome appears the most often in the sample space \[P_X(x_{\textnormal{mode}})\geq P_X(x).\]
        \item Median: The outcome which separates the higher half from  the lower half of a sample space \[P_X[X \leq x_{\textnormal{med}}] \geq 1/2, \qquad \qquad P_X[X \geq x_{\textnormal{med}}] \geq 1/2.\]
        \item (Arithmetic) mean/Expectation:  The sum of all the outcomes divided by the number of outcomes \[\mu_x = \bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i.\]
    \end{enumerate}
\end{definition}

\begin{definition}
    [Expected Value]
    The expected value of a discrete random variable $X$ with \textnormal{PMF} $P_X(x)$ is
    \begin{align}
        \E{X}&=\sum_{x\in S_X}xP_X(x). \tag{First Moment of $X$}\\
        \E{X^2}&=\sum_{x\in S_X}x^2P_X(x). \tag{Second Moment of $X$}
    \end{align}
\end{definition}

\begin{theorem}
    [Important Expectations]
    Here are some important expectations:
    \begin{enumerate}
        \item Bernoulli($p$): \[\E{X}=0\cdot P_X(0)+1\cdot P_X(1)=0(1-p)+1(p)=p.\]
        \item Binomial($n$, $p$): \[\E{X}=np.\]
        \item Poisson($\alpha$): \[\E{X}=\alpha.\]
        \item Geometric($p$): \[\E{X}=p\cdot 1 + (1-p)\cdot (1+\E{X})\Rightarrow 1/p.\]
        \item Pascal($k$, $p$): \[\E{X}=k/p.\]
        \item Discrete Uniform($k$, $l$): \[\E{X}=(k+l)/2.\]
    \end{enumerate}
\end{theorem}

\textbf{Note:}{
    \begin{enumerate}
        \item From an engineering perspective, \textbf{Mean (including Expectations, etc.)} is numerically easier to calculate, either using human brain or computers, than Mode and Median, when the sample space is humongous.
        \item In most cases, average, mean and expectation refer to the same concept.
    \end{enumerate}
}

\section{Derived Random Variable and Variance}
\begin{theorem}
    [Derived Random Variable]
    Given random variable $X$, let $Y = g(X)$, then
    \begin{enumerate}
        \item $P_Y(y) = P[Y=y] = P[Y=g(x)] = P[g^{-1}(Y)=g^{-1}(g(x))] = P[X=x] = P_X(x)$
        \item $\E{Y} = \sum yP_Y(y) = \sum g(x)P_X(x)$
        \item $\E{X-\mu_x}=\sum_{x\in S_X}(x-\mu_x)P_X(x)=\sum_{x\in S_X}xP_X(x)-\mu_x\sum_{x\in S_X}P_X(x)=\E{X}-\mu_x\cdot 1=0$
        \item $\E{aX+b}=a\E{X}+b \Rightarrow \E{b}=\E{0\cdot X+b}=b$
    \end{enumerate}
\end{theorem}

\begin{definition}
    [Variance and Standard Deviation]
    For random variable $X$, the variance ($\sigma_x^2$) is defined as
    \begin{align*}
        \sigma_x^2
        &= \Var[X] \\
        &= \E{(X-\mu_x)^2} \\
        &= \E{X^2-2\mu_x X+\mu_x^2} \\
        &= \E{X^2}-2\mu_x\E{X}+\E{\mu_x^2} \\
        &= \E{X^2}-2\mu_x^2+\mu_x^2 \\
        &= \E{X^2}-\mu_x^2
    \end{align*}
    and the standard deviation ($\sigma_x$) is defined as
    \begin{equation*}
        \sigma_x = \sqrt{\Var[X]}.
    \end{equation*}
\end{definition}

\begin{theorem}
    The variance of a random variable $X$ with has the following properties:
    \begin{enumerate}
        \item $\Var[X]\geq 0$
        \item $\Var[aX+b]=a^2\Var[X]$
    \end{enumerate}
\end{theorem}

\begin{theorem}
    [Important Variance]
    Here are some important variances:
    \begin{enumerate}
        \item Bernoulli($p$): \[\Var[X]=p(1-p).\]
        \item Binomial($n$, $p$): \[\Var[X]=np(1-p).\]
        \item Poisson($\alpha$): \[\Var[X]=\alpha.\]
        \item Geometric($p$): \[\Var[X]=(1-p)/p^2.\]
        \item Pascal($k$, $p$): \[\Var[X]=k(1-p)/p^2.\]
        \item Discrete Uniform($k$, $l$): \[\Var[X]=(l-k)(l-k+2)/12.\]
    \end{enumerate}
\end{theorem}
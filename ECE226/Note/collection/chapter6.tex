\chapter{Derived Probability Models}

\section{Functions of Discrete Random Variables}
\begin{theorem}
    For discrete random variables $X$ and $Y$, the derived random variable $W=g(X,Y)$ has \textnormal{PMF}
    \[P_W(w)=\sum_{\set{(x,y) \mid g(x,y)=w}}P_{X,Y}(x,y).\]
\end{theorem}

\section{Functions of Continuous Random Variables}
\begin{theorem}\label{thm:pdf_of_derived_rv}
    For continuous random variables $X$ and $Y$, the \textnormal{PDF} of derived random variable $W=g(X,Y)$ can be derived as
    \begin{enumerate}
        \item Find the \textnormal{CDF} $F_W(w)=P_W(W\leq w)$.
        \item Find the \textnormal{PDF} $f_W(w)=\diff F_W(w)/\diff w$.
    \end{enumerate}
\end{theorem}

\subsection{Functions of One Continuous Random Variable}
\begin{theorem}
    If $W=aX$ where $a>0$ is a constant, then
    \[F_W(w)=F_X(w/a) \qquad f_W(w)=\frac{1}{a}f_X(\frac{w}{a}).\]
    \begin{enumerate}
        \item If $X$ is $\Unif(b,c)$, then $W$ is $\Unif(ab,ac)$.
        \item If $X$ is $\Expv(\lambda)$, then $W$ is $\Expv(\lambda/a)$.
        \item If $X$ is $\Erlang(k,\lambda)$, then $W$ is $\Erlang(k,\lambda/a)$.
        \item If $X$ is $\Gaus(\mu,\sigma)$, then $W$ is $\Gaus(a\mu,a\sigma)$.
    \end{enumerate}
\end{theorem}

\begin{theorem}
    If $W=X+b$, where $b$ is a constant,
    \[F_W(w)=F_X(w-b) \qquad f_W(w)=f_X(w-b).\]
\end{theorem}

\begin{theorem}
    Let $U$ be a $\Unif(0,1)$ random variable and let $F(x)$ denote a \textnormal{CDF} with an inverse $F^{-1}(u)$ defined for $0<u<1$. Then $X=F^{-1}(U)$ is a random variable with \textnormal{CDF} $F_X(x)=F(x)$.
\end{theorem}

\subsection{Functions of Joint Continuous Random Variables}
Following the same idea as in \cref{thm:pdf_of_derived_rv}, we can easily derive the PDF of derived random variable $W=g(X,Y)$ when the function is linear. It is more complex for other functions.

\begin{theorem}
    For continuous random variables $X$ and $Y$, the \textnormal{CDF} of $W=g(X,Y)$ is
    \[F_W(w)=\P{W\leq w}=\iint\limits_{\set{(x,y) \mid g(x,y)\leq w}}f_{X,Y}(x,y)\diff x\diff y.\]
\end{theorem}

\begin{corollary}
    For continuous random variables $X$ and $Y$, the \textnormal{CDF} of $W=\max(X,Y)$ is
    \[F_W(w)=F_{X,Y}(w,w)=\int_{-\infty}^{w}\int_{-\infty}^{w}f_{X,Y}(x,y)\diff x\diff y.\]
\end{corollary}
\textbf{Hint:} $\set{\max(X,Y)\leq w}=\set{\set{X\leq w}\cap\set{Y\leq w}}.$

\begin{theorem}
    [The sum of two random variables]
    The \textnormal{PDF} of $W=X+Y$ is
    \[f_W(w)=\int_{-\infty}^{\infty}f_{X,Y}(x,w-x)\diff x = \int_{-\infty}^{\infty}f_{X,Y}(w-y,y)\diff y.\]
\end{theorem}

\begin{corollary}
    When $X$ and $Y$ are independent, the \textnormal{PDF} of $W=X+Y$ is
    \[f_W(w)=\int_{-\infty}^{\infty}f_{X}(x)f_Y(w-x)\diff x = \int_{-\infty}^{\infty}f_{X}(w-y)f_Y(y)\diff y.\]
\end{corollary}

\section{Sum (\ie, Linear Combinations) of Random Variables}
\subsection{Basic Properties}
\begin{definition}
    Random Variables of the form
    \[W_n=X_1+X_2+\cdots+X_n\]
    are called \textbf{sums of random variables}.
\end{definition}

\begin{theorem}
    [Expected Values of Sums]
    A generalized version of \cref{cor:linearity}
    \[\E{W_n}=\E{X_1+X_2+\cdots+X_n}=\E{X_1}+\E{X_2}+\cdots+\E{X_n}.\]
\end{theorem}

\begin{theorem}
    [Variance of Sums]
    A generalized version of \cref{thm:variance_of_sum}
    \[\Var[W_n]=\sum_{i=1}^{n}\Var[X_i]+2\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\Cov[X_i,X_j].\]
\end{theorem}

\begin{corollary}
    When $X_1,X_2,\ldots,X_n$ are uncorrelated, the variance of the sum is
    \[\Var[W_n]=\sum_{i=1}^{n}\Var[X_i].\]
\end{corollary}

\subsection{Methods of Generating Functions}
It is quite intuitive to consider an experimental random variable as the linear combination of many well-defined random variables. However, it is not easy to calculate the moments related information (\eg mean, variance, \etc) in such a case. The generating function method provides a way to solve this problem.
\begin{definition}
    [Probability Generating Function]
    If $X$ is a \textbf{non-negative discrete} random variable, then the probability generating function of $X$ is defined as
    \[G_X(z)=\E{z^X}=\sum_{x=0}^{\infty}z^xP_X(x).\]
    You can think of it as the \textit{Z-Transform} of $P_X(x)$.
\end{definition}

\begin{definition}
    [Moment Generating Function]
    For a random variable $X$, if the \textbf{moment generating function} is existed, it is defined as
    \begin{align*}
        M_X(t)&=\E{e^{tX}}\\
        &=\begin{cases}
            \sum_{x\in S_X}e^{tx}P_X(x) & \text{if $X$ is discrete}\\
            \int_{-\infty}^{\infty}e^{tx}f_X(x)\diff x & \text{if $X$ is continuous}
        \end{cases}
    \end{align*}
    You can think of it as the \textit{Laplace Transform} of $P_X(x)$/$f_X(x)$. Note that $M_X(t)=G_X(e^t)$.
\end{definition}

\begin{table}[H]
    \centering
    \renewcommand{\arraystretch}{2}
    \begin{tabular}{|l|l|}
    \hline
    Random Variables            & MGF                                               \\ \hline
    Bernoulli($p$)              & $1-p+pe^t$                                 \\ \hline
    Binomial($n$,$p$)           & $(1-p+pe^t)^n$                             \\ \hline
    Geometric($p$)              & $\frac{pe^t}{1-(1-p)e^t}$                  \\ \hline
    Pascal($k$,$p$)             & $\left(\frac{pe^t}{1-(1-p)e^t}\right)^k$   \\ \hline
    Poisson($\alpha$)           & $e^{\alpha(e^t-1)}$                        \\ \hline
    Discrete Uniform($k$,$l$)   & $\frac{e^{kt}-e^{(l+1)t}}{e^t-1}$          \\ \hline
    Constant($c$)               & $e^{ct}$                                   \\ \hline
    Continuous Uniform($k$,$l$) & $\frac{e^{kt}-e^{lt}}{t(l-k)}$             \\ \hline
    Exponential($\lambda$)      & $\frac{\lambda}{\lambda-t}$                \\ \hline
    Erlang($n$,$\lambda$)       & $\left(\frac{\lambda}{\lambda-t}\right)^n$ \\ \hline
    Gaussian($\mu$,$\sigma$)    & $e^{\mu t+\frac{1}{2}\sigma^2t^2}$         \\ \hline
    \end{tabular}
    \caption{Common MGF}
\end{table}

\begin{theorem}
    A random variable $X$ with \textnormal{MGF} $M_X(t)$ has $n$th moment
    \[\E{X^n}=M_X^{(n)}(0)=\frac{\diff^n M_X(t)}{\diff t^n}\bigg\rvert_{t=0}.\]
\end{theorem}

\begin{theorem}
    The \textnormal{MGF} of $Y=aX+b$ is $M_Y(t)=e^{bt}M_X(at)$.
\end{theorem}

\begin{proof}
    \begin{align*}
        M_Y(t)&=\E{e^{tY}}\\
        &=\E{e^{t(aX+b)}}\\
        &=e^{bt}\E{e^{(at)X}}\\
        &=e^{bt}M_X(at).
    \end{align*}
\end{proof}

\begin{corollary}
    [Central Moment]
    The \textnormal{MGF} of $Y=X-\mu$ is,
    \[M_Y(t)=e^{-\mu t}M_X(t).\]
\end{corollary}

\begin{corollary}
    [standardized Moment]
    The \textnormal{MGF} of $Y=(X-\mu)/\sigma$ is,
    \[M_Y(t)=e^{-\mu t/\sigma}M_X(t/\sigma).\]
\end{corollary}

\begin{remark}
    Let's clarify a few concepts.
    \begin{enumerate}
        \item Centering: $Y=X-\mu$. Align the distribution to the origin.
        \item Normalization: $Y=X/\sigma$. Eliminate the variance. This make the distribution independently of any linear change of scale. It is important when dealing with joint distributions.
        \item Standardization: $Y=(X-\mu)/\sigma$. Normalized Centering.
    \end{enumerate}
\end{remark}

\textbf{Note:} If the \textnormal{MGF} exists, it uniquely determines the probability distribution. It is powerful because the calculation of moments becomes derivative instead of integral. \textnormal{MGF} fails to calculate the probability.

\begin{theorem}
    [\textnormal{MGF} of the Sum of Independent Random Variables]\label{thm:MGF_of_sum_of_independent_rv}
    If $X_1,X_2,\ldots,X_n$ are independent random variables with \textnormal{MGF}s $M_{X_1}(t),M_{X_2}(t),\ldots,M_{X_n}(t)$, then the \textnormal{MGF} of $W_n=X_1+X_2+\cdots+X_n$ is
    \[M_{W_n}(t)=M_{X_1}(t)M_{X_2}(t)\cdots M_{X_n}(t).\]
    If $X_1,X_2,\ldots,X_n$ are also identically distributed, then
    \[M_{W_n}(t)=\left[M_{X}(t)\right]^n.\]
\end{theorem}

\begin{definition}
    [Random Sums of \iid Random Variables]
    Let $N$ be a random variable with \textnormal{PMF} $P_N(n)$, and let $X_1,X_2,\ldots$ be a sequence of independent random variables. Then the random variable $W=X_1+X_2+\cdots+X_N$ is called a \textbf{random sum of \iid random variables}.
\end{definition}

\begin{theorem}
    [\textnormal{MGF} of Random Sums of \iid Random Variables]
    Let $\set{X_i}$ be a sequence of \iid random variables with \textnormal{MGF} $M_X(t)$, and let $N$ be a non-negative integer-valued random variable independent of $\set{X_i}$ with \textnormal{PMF} $P_N(n)$. Then the \textnormal{MGF} of $W=X_1+X_2+\cdots+X_N$ is
    \[M_W(t)=M_N(\ln M_X(t)).\]
\end{theorem}

\begin{theorem}
    [Expectation of Random Sums of \iid Random Variables]
    \[\E{W}=\E{N}\E{X}.\]
\end{theorem}

\begin{theorem}
    [Variance of Random Sums of \iid Random Variables]
    \[\Var[W]=\Var[X]\E{N}+\E{X}^2\Var[N].\]
\end{theorem}

\begin{definition}
    [Cumulant Generating Function]
    The \textbf{cumulant} of a random variable $X$ is defined as the natural logarithm of the \textnormal{MGF} of $X$:
    \[K_X(t)=\ln M_X(t).\]
    The $n$-th-order cumulant $K_n$ is
    \[K_n(X)=K^{(n)}(0).\]
\end{definition}

\textbf{Note:} In some cases theoretical treatments of problems in terms of cumulants are simpler than those using moments. In particular, when two or more random variables are statistically independent, the $n$-th-order cumulant of their sum is equal to the sum of their $n$-th-order cumulants, which is preferred over the product of their $n$-th-order moments.

\textbf{Note:} The third and higher-order cumulants of a \textbf{Gaussian} distribution are zero, and it is the only distribution with this property.

\begin{theorem}
    [The First Three Cumulants]
    For random variable $X$,
    \begin{enumerate}
        \item $K_1(X)=\E{X}$ which is the mean.
        \item $K_2(X)=\Var[X]$ which is the variance or the second central moment.
        \item $K_3(X)=\E{(X-\E{X})^3}$ is the third central moment.
    \end{enumerate}
\end{theorem}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|ccc|cc|}
    \hline
    \multirow{2}{*}{Order} & \multicolumn{3}{c|}{Moment}                                          & \multicolumn{2}{c|}{Cumulant}            \\ \cline{2-6}
     & \multicolumn{1}{c|}{Raw} & \multicolumn{1}{c|}{Central} & Standarized & \multicolumn{1}{c|}{Raw} & Normalized \\ \hline
    1                      & \multicolumn{1}{c|}{Mean} & \multicolumn{1}{c|}{0}        & 0        & \multicolumn{1}{c|}{Mean}     & -        \\ \hline
    2                      & \multicolumn{1}{c|}{-}    & \multicolumn{1}{c|}{Variance} & 1        & \multicolumn{1}{c|}{Variance} & 1        \\ \hline
    3                      & \multicolumn{1}{c|}{-}    & \multicolumn{1}{c|}{-}        & Skewness & \multicolumn{1}{c|}{-}        & Skewness \\ \hline
    \end{tabular}
    \caption{Relation between moments and cumulants on the first three orders:}
    \label{tab:my-table}
\end{table}

\begin{definition}
    [Characteristic Function]
    For a random variable $X$, the \textbf{characteristic function} is defined as
    \begin{align*}
        \phi_X(t)&=\E{e^{itX}}\\
        &=\begin{cases}
            \sum_{x\in S_X}e^{itx}P_X(x) & \text{if $X$ is discrete}\\
            \int_{-\infty}^{\infty}e^{itx}f_X(x)\diff x & \text{if $X$ is continuous}
        \end{cases}
    \end{align*}
    You can think of it as the \textit{Fourier Transform} of $P_X(x)$/$f_X(x)$. Note that $\phi_X(t)=M_X(it)$. The characteristic function is always defined.
\end{definition}

\section{Central Limit Theorem}\label{sec:CLT}
In this part of the lecture, we will see a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behavior that is essentially unchanging when items far enough into the sequence are studied. This is the fundamental theory that why probability (\ie a measure of uncertainty) can be used to model the deterministic world.

\begin{theorem}
    [Central Limit Theorem]\label{thm:CLT}
    Given $X_1, X_2,\ldots$ a sequence of \iid random variables with expected value $\mu_X$ and variance $\sigma_X^2$, the \textnormal{CDF} of $Z_n=(\sum_{i=1}^{n}X_i-n\mu_X)/\sqrt{n\sigma_X^2}$ has the property
    \[\lim_{n\rightarrow \infty}F_{Z_n}(z)=\Phi(z).\]
    That is the \textbf{CDF} of ``sum of standardized \iid random variables (not necessary Gaussian)'' converges to the standard Gaussian random variable as $n\rightarrow\infty$. Alternatively, we can express it as
    \[\lim_{n\rightarrow \infty}\frac{n(\bar{X}-\mu_X)}{\sqrt{n}\sigma_X}\sim \Gaus(0, 1).\]
\end{theorem}

\begin{proof}
    \begin{align*}
        \E{\exp\braces*{it\cdot \frac{\sum_{i=1}^{n}X_i-n\mu_X}{\sqrt{n\sigma_X^2}}}}
        &= \E{\exp\braces*{\sum_{i=1}^{n} it\cdot \frac{X_i-\mu_X}{\sqrt{n\sigma_X^2}}}} \\
        &= \braces*{\E{\exp\braces*{it\cdot \frac{X_i-\mu_X}{\sqrt{n\sigma_X^2}}}}}^n \\
        &= \braces*{1-\frac{t^2}{2n}+o\parens*{\frac{t^2}{2n}}}^n \\
        &\rightarrow \exp\braces*{-\frac{1}{2}t^2} \sim \Gaus(0,1).
    \end{align*}
\end{proof}

\begin{corollary}
    With \cref{thm:CLT}, we can express the $W_n=X_1+X_2+\cdots+X_n$ with \iid $X_i$ as
    \[W_n=\sqrt{n\sigma_X^2}Z_n+n\mu_X.\]
    The \textnormal{CDF} of $W_n$ is
    \[F_{W_n}(w)=\P{\sqrt{n\sigma_X^2}Z_n+n\mu_X\leq w}=\P{Z_n\leq \frac{w-n\mu_X}{\sqrt{n\sigma_X^2}}}=\Phi\left(\frac{w-n\mu_X}{\sqrt{n\sigma_X^2}}\right).\]
\end{corollary}

\begin{theorem}
    [De Moivre-Laplace Theorem]\label{thm:DeMoivreLaplace}
    For a binomial random variable $X\sim \Binomv(n,p)$,
    \[\P{x_1\leq X\leq x_2}\approx \P{x_1-0.5\leq X\leq x_2+0.5}\approx \Phi\parens*{\frac{x_2+0.5-np}{\sqrt{np(1-p)}}}-\Phi\parens*{\frac{x_1-0.5-np}{\sqrt{np(1-p)}}}.\]
\end{theorem}

% \section{Family of Distributions derived using \textnormal{MGF}s}

% \begin{enumerate}
%     \item \textbf{Bernoulli}($p$){
%         \begin{align*}
%             M_X(t)
%             &= \E{e^{tX}}\\
%             &= e^{t\cdot 0}P_X(0)+e^{t\cdot 1}P_X(1)\\
%             &= 1-p+pe^t.
%         \end{align*}
%     }
%     \item \textbf{Binomial}($n$,$p$): The sum of $n$ \iid Bernoulli($p$).{
%         \begin{align*}
%             M_X(t)
%             &= \E{e^{tX}}\\
%             &= \E{e^{t(X_1+X_2+\cdots+X_n)}}\\
%             &= \E{e^{tX_1}e^{tX_2}\cdots e^{tX_n}}\\
%             &= \E{e^{tX_1}}\E{e^{tX_2}}\cdots \E{e^{tX_n}}\\
%             &= \left(1-p+pe^t\right)^n.
%         \end{align*}
%         This can also be done using \cref{thm:MGF_of_sum_of_independent_rv}.
%     }
%     \item \textbf{Poisson}($\alpha$): \textbf{Binomial}($n$,$p$) with $n\rightarrow\infty$ and $\alpha=np${
%         \begin{align*}
%             M_X(t)
%             &= \lim_{n\rightarrow\infty}\parens*{1-\frac{\alpha}{n}+\frac{\alpha}{n}e^t}^n \\
%             &= \lim_{n\rightarrow\infty}\parens*{1+\frac{\alpha\parens*{e^t-1}}{n}}^n \\
%             &= e^{\alpha\parens*{e^t-1}}.
%         \end{align*}
%     }
%     \item \textbf{Geometric}($p$){
%         \begin{align*}
%             &M_X(t)
%             = p\cdot e^{t\cdot 1}+(1-p)\E{e^{t(X+1)}}
%             = pe^t+(1-p)M_X(t)e^t\\
%             \Rightarrow
%             &M_X(t)=\frac{pe^t}{1-(1-p)e^t}.
%         \end{align*}
%     }
%     \item \textbf{Pascal}($k$, $p$): The sum of $k$ \iid \textbf{Geometric}($p$).{
%         \begin{align*}
%             M_X(t)
%             &= \parens*{\frac{pe^t}{1-(1-p)e^t}}^k.
%         \end{align*}
%     }
%     \item \textbf{Gamma}($\alpha=k$, $\alpha$): The sum of $k$ \iid \textbf{Exponential}($\alpha$).{
%         \begin{align*}
%             M_X(t)
%             &= \parens*{\frac{\alpha}{\alpha-t}}^k.
%         \end{align*}
%     }
% \end{enumerate}
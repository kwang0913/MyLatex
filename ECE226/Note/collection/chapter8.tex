\chapter{Introduction of Information Theory}

\begin{definition}
    [bit]
    The \textbf{bit} is the unit of information. It is representing a binary choice between two alternatives (such as true/false, on/off, or 0/1 in binary code). Therefore, if an event has a probability of $1/2$, it carries $1$ bit of information, because observing the event eliminates one out of two equally likely possibilities. If the probability is $1/4$, it carries $2$ bits of information, and so on.
\end{definition}

\begin{definition}
    [self-information]
    The \textbf{self-information} of an event $X$ with \textnormal{PMF} $P_X(x)$ is defined as
    \[I(X)=-\log_2P_X(x).\]
    Consequently, the self-information of 1 bit (\ie, $p(x)=1/2$) is 1. The 2-based logarithmic is the most common choice in the information theory, but the natural logarithm is also used in some cases.
\end{definition}

\begin{definition}
    [Shannon-entropy]
    The \textbf{Shannon-entropy} of a discrete random variable $X$ with \textnormal{PMF} $P_X(x)$ is defined as
    \[H(X)=-\sum_{x\in\mathcal{X}}P_X(x)\log_2P_X(x)=\sum_{x\in\mathcal{X}}P_X(x)I(x)=\E{I(X)}.\]
    The $\mathcal{X}$ is the sample space of $X$, which is the common notation in the information theory.
\end{definition}

\begin{definition}
    [Joint entropy]
    The \textbf{joint entropy} of two discrete random variables $X$ and $Y$ with \textnormal{PMF} $P_{X,Y}(x,y)$ is defined as
    \[H(X,Y)=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}P_{X,Y}(x,y)\log_2P_{X,Y}(x,y)=\E{I(X,Y)}.\]
\end{definition}

\begin{definition}
    [Conditional entropy]
    The \textbf{conditional entropy} of two discrete random variables $X$ and $Y$ with \textnormal{PMF} $P_{XY}(x,y)$ is defined as
    \begin{align*}
        H(X|Y)
        &= \sum_{y\in \mathcal{Y}}P_Y(y)H(X|Y=y)\\
        &= -\sum_{y\in\mathcal{Y}}P_Y(y)\sum_{x\in\mathcal{X}}P_{X|Y}(x|y)\log_2P_{X|Y}(x|y)\\
        &= -\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}P_{X,Y}(x,y)\log_2P_{X|Y}(x|y) \\
        &= \E{I(X\mid Y)}.
    \end{align*}
\end{definition}

\begin{theorem}
    [Chain Rule of Entropy]\label{thm:chainruleentropy}
    \[H(X,Y)=H(X)+H(Y|X)=H(Y)+H(X|Y).\]
\end{theorem}

The entropy of a random variable is a measure of the uncertainty of the
random variable; it is a measure of the amount of information required on
the average to describe the random variable. The relative entropy is a measure of the distance between two distributions or the inefficiency of assuming
that the distribution is q when the true distribution is p.

\begin{definition}
    [Relative entropy (Kullback-Leibler divergence)]
    The \textbf{relative entropy} of two \textnormal{PMF} $P_X(x)$ and $Q_X(x)$ is defined as
    \[D(P_X\|Q_X)=\sum_{x\in\mathcal{X}}P_X(x)\log_2\frac{P_X(x)}{Q_X(x)}.\]
\end{definition}

\begin{definition}
    [Mutual information]
    The \textbf{mutual information} of two discrete random variables $X$ and $Y$ is the KL-divergence between their joint distribution and their products (marginal) distributions.
    \begin{align*}
        I(X;Y)
        &= \sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}P_{X,Y}(x,y)\log_2\frac{P_{X,Y}(x,y)}{P_X(x)P_Y(y)}\\
        &D(P_{X,Y}\|P_X P_Y).
    \end{align*}
\end{definition}

\begin{theorem} [Mutual Information and Entropy]
    \begin{align*}
        I(X;Y) &= H(X)-H(X|Y)\\
        I(X;Y) &= H(Y)-H(Y|X)\\
        I(X;Y) &= H(X)+H(Y)-H(X,Y)\\
        I(X;Y) &= I(Y;X)\\
        I(X;X) &= H(X)
    \end{align*}
\end{theorem}

So mutual information is the reduction in the uncertainty of $X$ due to the knowledge of $Y$. If $X$ and $Y$ are independent, then $I(X;Y)=0$. This leads to the interpretation of independence as observing $Y$ does not reduce the uncertainty in $X$ if they are independent.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/MI.png}
    \caption{Relationship between entropy and mutual information.}
\end{figure}
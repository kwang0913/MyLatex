{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plan of studying Neural Tangent Kernel (NTK).\n",
    "\n",
    "- Study the Neural Tangent Library by Google\n",
    "- Reproduce several works published in 2019 focusing on the basic property of NTK and being familiar with the code\n",
    "- TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following paper contains the development of the [neural-tangents](https://github.com/google/neural-tangents) library\n",
    "\n",
    "- [Neural Tangents: Fast and Easy Infinite Neural Networks in Python](https://arxiv.org/abs/1912.02803)\n",
    "- [Fast Finite Width Neural Tangent Kernel](https://arxiv.org/abs/2206.08720)\n",
    "- [Infinite attention: NNGP and NTK for deep attention networks](https://arxiv.org/abs/2006.10540)\n",
    "- [On the infinite width limit of neural networks with a standard parameterization](https://arxiv.org/abs/2001.07301)\n",
    "- [Fast Neural Kernel Embeddings for General Activations](https://arxiv.org/abs/2209.04121)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic knowledge of Deep Neural Network(DNN) needed in understanding NTK\n",
    "\n",
    "- Neural Networks are build using principle of neuronal organization discovered by connection in the biological neural networks constituting animal brains. It is loosely modeled by stacking layers of neurons. Such architecture is also called Multi-Layer Perceptron (MLP).\n",
    "- [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) states that `1-layer` MLP has capability in modeling any function with the following condition:\n",
    "  - The input dimension is smaller than the number of neurons. Actually, it needs to be far less.\n",
    "  - The continuous function is in Euclidean Space\n",
    "- In practice, the width (number of neurons in a single layer) cannot go to infinity. To overcome this limitation, `n-layers` MLP (a.k.a. DNN) was invented with the following setup:\n",
    "  - Activation function between layers. Without activation function, `n-layers` MLP will be mathematically equivalent to `1-layer` MLP due to linearity. That says activation involves non-linearity for DNN.\n",
    "  - A typical DNN will have at least `2-layers` so that there will be a place for activation function in between layers. **That's why so many theories focusing on analyzing `2-layers` MLP architecture**.\n",
    "- Empirically, the DNN is more powerful than it sounds to be (i.e. beyond the Universal Approximation Theorem) as follows:\n",
    "  - DNN not only shows superior performance in approximation but also in prediction (generalization, beyond human accuracy).\n",
    "  - DNN is not limited to Euclidean Space\n",
    "- Some architectures are designed to further improve the performance of the DNN, empirically, with loosely theoretically analysis:\n",
    "  - Batch Normalization (faster convergence)\n",
    "  - Dropout (non-linearity for generalization)\n",
    "  - Initialization (Better overall performance)\n",
    "  - ReLU (Overcome Gradient Explosion)\n",
    "  - ResNet (Overcome Gradient Vanish)\n",
    "  - CNN (Computer Vision)\n",
    "  - Transformer (Human knowledge Prior)\n",
    "  - Diffusion (Mimic Optimal Transportation)\n",
    "  - RNN, Knowledge Distillation, etc.\n",
    "- Some Theory Framework (Trick?)\n",
    "  - Neural Ordinary Differential Equation and Neural Partial Differential Equation. Traditional deep neural networks can be thought of as discretized transformations of data across layers. Neural Differential Equations replace these discrete transformations with a continuous transformation.\n",
    "  - Mean Field Theory to analyze the Optimization Dynamic at Initialization.\n",
    "  - **`Neural Tangent Kernel to analyze the infinity width DNN`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Tangent Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

\documentclass[11pt]{article}
\usepackage[letterpaper, margin=1in]{geometry}


%%%% Header and Foot Information %%%%
\newcommand{\lefthead}{ECE 14:332:525}
\newcommand{\centerhead}{\textbf{Optimal Signal Processing}}
\newcommand{\righthead}{Spring 2023}
\newcommand{\leftbottom}{}
\newcommand{\centerbottom}{\thepage}
\newcommand{\rightbottom}{}

%%%% Load Format %%%%
\input{../Formats/format.tex}
\input{../Formats/Macro.tex}
% \allowpagebreaks
%%%% Document Information %%%%
\title{Project Report}
\author{Kailong Wang}
\date{\today}

\addbibresource{ece525.bib}

%%%% Start Document %%%%
\begin{document}
\maketitle

\begin{abstract}
This is the course report of ECE525 Optimal Signal Processing. The topic covered is the Combinations of Adaptive Filters--Performance and convergence properties.
\end{abstract}

\section{Introduction}
Adaptive filters form the foundation of numerous signal processing applications, with research focusing on enhancing their performance and properties.
Novel adaptive structures aim to improve performance trade-offs, such as convergence rate and steady-state performance, as well as integrating a priori knowledge into filter learning mechanisms.
However, when a priori knowledge is limited or imprecise, selecting the ideal filter structure and parameters can be challenging, potentially resulting in suboptimal performance.

One solution to this challenge is the combination of multiple adaptive filters with varying characteristics.
This approach mirrors the divide and conquer principle used in ensemble learning within the machine-learning community.
The focus of this report is on the review~\cite{Arenas_Garcia_2016} of key ideas and principles behind combination schemes, emphasizing design rules to derive the deterministic bounds that apply to individual sequences.

\subsection{Problem Formulation and Notation}
The adaptive filter in the MSE sense satisfy a linear regression model of the form
\begin{equation}
    d(n) = u(n)\trn w_o(n)+v(n),
\end{equation}
where $w_o(n)$ represents the (possibly) time-varying optimal solution, and $v(n)$ is a noise sequence (usually WSS), which is under the assumption of independent and identically distributed, and independent of $u(m)$, for all $n$, $m$.
The estimation of the optimal solution at time $n$ is usually solved by recursion of the form\begin{equation}
    w(n+1)=f[w(n), d(n), u(n), s(n)],
\end{equation}
where $f[\cdot]$ is the update function of the adaptive schemes and $s(n)$ represents any other state information that required to update the filter.

The following error variables are considered in evaluating and designing the adaptive filters:
\begin{itemize}
    \item Weight error: $\tilde{w}(n)=w_o(n)-w(n)$
    \item A priori filter error: $e_a(n)=u(n)\trn\tilde{w}(n)$
    \item Filter error: $e(n)=d(n)-u(n)\trn w(n)=e_a(n)+v(n)$
    \item Mean squared error (MSE): $\MSE(n)=\E{e^2(n)}$
    \item Excess MSE (EMSE): $\xi(n)=\E{e_a^2(n)}=\MSE(n)-\E{v^2(n)}$
    \item Mean squared deviation: $\MSD(n)=\E{\norm{\tilde{w}(n)}_2^2}$.
\end{itemize}
During the operation, adaptive filters normally converge to a steady-state regime in which the MSE decreases to some asymptotic value. Thus, for steady-state performance, the steady-state MSE, EMSE, and MSD are defined as their limiting values as $n$ increases. For instance, the steady-state EMSE is defined as
\begin{equation}
    \xi(\infty)=\lim_{n\rightarrow\infty}\E{e_a^2(n)}.
\end{equation}

\subsection{A Basic Combination of Two Adaptive Filters}
The most simple combination scheme incorporates two adaptivefilters.
Both adaptive filters have access to the same input and reference signals and produce their individual estimates of the optimum weight vector, $w_o(n)$.
The goal of the combination layer is to learn which filter component is performing better dynamically at any particular time, assigning them weights to optimize the overall performance.
The affine and convex combinations of the two filters are defined as
\begin{equation}\label{eq:affine_combination}
    y(n)=\lambda(n)y_1(n)+(1-\lambda(n))y_2(n),
\end{equation}
where $y_n=u\trn(n)w_i(n),\ i=1,2$, $\lambda(n) \in [0,1]$ is the convex combination parameter, and $w_i(n)$ is the weight vector of each individual filter. Accordingly, the estimated weight vector $w(n)$, the error $(e(n))$, and the a priori error $e_a(n)$ of the combination scheme have the similar form. For the purpose of the performance, it is crucial to design mechanisms to learn the $\lambda(n)$. The~\cref{sec:estimating} will discuss the estimation of the combination parameter.


\subsection{Optimum mixing parameter and combination performance}
Here we derive the expression for the optimal mixing parameter in the affine and convex cases, in the sense of minimizing the MSE of the combination.
The EMSE of the~\cref{eq:affine_combination} is
\begin{equation}\label{eq:EMSE_affine}
    \xi(n)= \E{e_a^2(n)} =
    \lambda^2(n)\xi_1(n)+(1-\lambda(n))^2\xi_2(n)+2\lambda(n)[1-\lambda(n)]\xi_{12}(n),
\end{equation}
where the individual EMSE $\xi_i(n)=\E{e_{a,i}^2(n)}$ and the cross-EMSE $\xi_{12}(n)=\E{e_{a,1}(n)e_{a,2}(n)}$.
Equation~\eqref{eq:EMSE_affine} is a quadratic function of $\lambda(n)$, and its minimum is obtained at affine case
\begin{equation}
    \lambda_{\mathsf{aff}}(n)=\frac{\xi_2(n)-\xi_{12}(n)}{\xi_1(n)+\xi_2(n)-2\xi_{12}(n)} = \frac{\triangle\xi_2(n)}{\triangle\xi_1(n)+\triangle\xi_{2}(n)},
\end{equation}
where $\triangle\xi_i(n)=\xi_i(n)-\xi_{12}(n), i=1,2$.
Or at convex case
\begin{equation}
    \lambda_{\mathsf{cvx}}(n)=\frac{\triangle\xi_2(n)}{\triangle\xi_1(n)+\triangle\xi_{2}(n)}\bigg\rvert_0^1,
\end{equation}
where the vertical line denotes truncation to the indicated values.
With these expressions, some theoretical performance of optimally combined adaptive filters can be derived and it is included in the~\cite{Arenas_Garcia_2016}.

\section{Cross-EMSE of LMS and RLS filters}
The cross-EMSE plays an important role in clarifying the performance regime of a combination of two adaptive filters. In this section, we review some main results for LMS and RLS filters.

\subsection{Combination of two LMS filters with different step sizes}
The combination of two filers with different adaptation speeds is a common practice in adaptive filtering. Let's take a look at the combination of two LMS filters as an example. 
Using the tracking model
\begin{equation}
    w_o(n)=w_o(n-1)+q(n),
\end{equation}
where $q(n)$ is the weight change vector at every step, which is assumed to have zero-mean and covariance $Q=\E{q(n)q\trn(n)}$, the theoretical steady-state EMSE depends on $\tr(Q)$. To facilitate the comparison among filters, we will recur to the normalized squared deviation (NSD)
\begin{equation}
    \NSD(n)=\frac{\xi_i(\infty)}{\xi_o(\infty)}.
\end{equation}
With several examples provided in the~\cite{Arenas_Garcia_2016}, we concluded that
\begin{itemize}
    \item The tracking ability of the adaptive implementation can be improved by using the combination structure. The paper further discuss the superior performance of the combination regime comparing with the variable step-size (VSS) algorithm.
    \item The combination of two LMS filters with different step sizes cannot outperform the best individual filter. Thus, with enough prior knowledge, the use of individual filter is preferable.
\end{itemize}

\subsection{Improving the tracking performance of LMS and RLS filters}
Next we see the combination of LMS and RLS filter. As explained in the paper, LMS will outperform RLS if $Q$ is proportional to the autocorrelation matrix of the input signal, $R$, and the opposite will occur when $Q \propto R\inv$. Consider a synthetic example where $Q$ is a mixture of of $R$ and $R\inv$,
\begin{equation}
    Q=10^{-5}\bracks*{\alpha\frac{R}{\tr(R)}+(1-\alpha)\frac{R\inv}{\tr(R\inv)}},
\end{equation}
the optimal EMSE for RLS filters can be larger or smaller than the optimal EMSE for LMS filters. I hypothesis that the solution space of LMS and RLS are not overlapping so the joint solution space of the combination of LMS and RLS is larger than the individual solution space. Thus, the combination of LMS and RLS can outperform the best individual filter, which is the same as the computational learning scenario.

\subsection{Estimating the combination parameter}\label{sec:estimating}
Since the optimum linear combiner is unrealizable, many practical algorithms have been proposed to adjust the mixing parameter in convex and affine combinations. Rewrite the~\cref{eq:affine_combination} as
\begin{equation}
    y(n) = y_2(n)+\lambda(n)[y_1(n)-y_2(n)],
\end{equation}
we can reinterpret the adaptation of $\lambda(n)$ as a ``second layer'' adaptive filter of length one, so that in principle any adaptive rule can be used for adjusting the mixing parameter. However, the time-varying property of $[y_1(n)-y_2(n)]$ makes the optimization of such a filter challenger than usual. In the affine case, the optimal solution of the combined filter may stay away from the optimum EMSE even with carefully selected parameters. When there is no enough prior knowledge, the convex combination schemes recur to activation functions to keep the mixing parameter in the range of interest, which simplifies the selection of the step size. A \textbf{power normalized updating rule} is proposed in the paper to obtain a more robust scheme.

\section{Combination of several adaptive filters}
We have considered the combinations of two adaptive filters. Intuitively, combining $K$ adaptive filters makes it possible to further increase the robustness and versatility of combination schemes. For instance
\begin{itemize}
    \item More filters can introduce new step size values, which can be used to improve the tracking ability of the combination scheme.
    \item More filters can alleviate several comprises simultaneously. 
\end{itemize}

We can select the combination of any two elements at once and incorporates the rest one by one. In terms of the mathematical expression,
\begin{equation}
    y(n) =\lambda_{21}(n)\braces*{\lambda_{11}(n)y_1(n)+\bracks*{1-\lambda_{11}(n)}y_2(n)}+\bracks*{1-\lambda_{21}(n)}\braces*{\lambda_{12}(n)y_3(n)+\bracks*{1-\lambda_{12}(n)}y_4(n)}.
\end{equation}
Or in a one-layer schemes
\begin{equation}
    y(n)=\sum_{k=1}^{K-1}\lambda_k(n)y_k(n)+\bracks*{1-\sum_{k=1}^{K-1}\lambda_k(n)}y_K(n).
\end{equation}
One useful characteristic of hierarchical schemes is its ability to selects the best combination parameters of both competing models.

\section{Calculation Cost and Applications}
The rest of the paper introduce the strategies to reduce the computation cost in designing the combination schemes. For example, use a low-cost filter as companion to a high-cost one not only improve the overall performance but also reduce the computational cost of the high-cost one, which yield a systematic computation cost reduction. The paper also introduces several applications of the combined filters. For example, by track the changes in the modality of different kinds of signals, we can determine whether a signal is generated through a linear or a nonlinear system, which is not viable through the single filter. The implementations are also discussed in designing adaptive blind equalization, sparse system identification, acoustic echo cancellation. 

\printbibliography
\end{document}
